{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae4d326",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q unsloth[colab-new] datasets transformers accelerate bitsandbytes wandb\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import gc\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "class MedicalConfig:\n",
    "    \"\"\"Configuration tailored for medical question-answering fine-tuning\"\"\"\n",
    "\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "\n",
    "    max_seq_length = 2048  \n",
    "\n",
    "    lora_r = 16            \n",
    "    lora_alpha = 16        \n",
    "    lora_dropout = 0       \n",
    "\n",
    "    batch_size = 2                 \n",
    "    gradient_accumulation = 4      \n",
    "\n",
    "    epochs = 3\n",
    "    learning_rate = 2e-4\n",
    "    weight_decay = 0.01\n",
    "    warmup_ratio = 0.1\n",
    "\n",
    "    fp16 = True\n",
    "    bf16 = is_bfloat16_supported()\n",
    "    seed = 42  # reproducibility\n",
    "\n",
    "config = MedicalConfig()\n",
    "\n",
    "def check_system_readiness():\n",
    "    \"\"\"Check if your Colab setup is ready for medical AI training\"\"\"\n",
    "\n",
    "    print(\"Medical AI Training System Check\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu = torch.cuda.get_device_properties(0)\n",
    "        memory_gb = gpu.total_memory / 1e9\n",
    "        print(f\"GPU Ready: {gpu.name}\")\n",
    "        print(f\"GPU Memory: {memory_gb:.1f} GB\")\n",
    "        if memory_gb < 12:\n",
    "            print(\" Recommendation: Reduce batch_size or max_seq_length if OOM\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"No GPU found! Enable GPU: Runtime → Change runtime type → T4 GPU\")\n",
    "        return False\n",
    "\n",
    "if not check_system_readiness():\n",
    "    raise SystemError(\"Please enable GPU before continuing\")\n",
    "\n",
    "def load_medical_dataset(filename=\"medDataset_processed.csv\"):\n",
    "    \"\"\"Load your medical dataset with care and attention to quality\"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    print(f\"Loaded {len(df)} medical Q&A pairs\")\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        q, a = str(row[\"Question\"]).strip(), str(row[\"Answer\"]).strip()\n",
    "        if len(q) < 10 or len(a) < 20:\n",
    "            continue\n",
    "        records.append({\"question\": q, \"answer\": a, \"qtype\": row.get(\"qtype\", \"general\")})\n",
    "    print(f\"Prepared {len(records)} high-quality medical Q&A pairs\")\n",
    "    return Dataset.from_list(records)\n",
    "\n",
    "def create_medical_prompt(question, answer, tokenizer):\n",
    "    \"\"\"Generate ChatML-style conversation for Llama 3.2 Instruct\"\"\"\n",
    "    conv = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a knowledgeable medical assistant providing helpful information.\"},\n",
    "        {\"role\": \"user\",   \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": answer}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "def prepare_training_data(dataset, tokenizer):\n",
    "    def _format(batch):\n",
    "        texts = []\n",
    "        for q, a in zip(batch[\"question\"], batch[\"answer\"]):\n",
    "            texts.append(create_medical_prompt(q, a, tokenizer))\n",
    "        return {\"text\": texts}\n",
    "\n",
    "    processed = dataset.map(_format, batched=True, remove_columns=dataset.column_names)\n",
    "    print(f\"Formatted {len(processed)} examples for medical training\")\n",
    "    return processed\n",
    "\n",
    "def initialize_medical_model():\n",
    "    \"\"\"Load and configure Llama 3.2 1B for medical fine-tuning\"\"\"\n",
    "    print(\"Loading Llama 3.2 1B for medical training...\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=config.model_name,\n",
    "        max_seq_length=config.max_seq_length,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=config.lora_r,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=config.seed,\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    total = model.num_parameters()\n",
    "    trainable = model.num_parameters(only_trainable=True)\n",
    "    print(f\"Total params: {total:,}, Trainable: {trainable:,} ({100*trainable/total:.1f}%)\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def setup_medical_training():\n",
    "    return TrainingArguments(\n",
    "        output_dir=\"./medical-ai-results\",\n",
    "        num_train_epochs=config.epochs,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation,\n",
    "        learning_rate=config.learning_rate,\n",
    "        weight_decay=config.weight_decay,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=config.warmup_ratio,\n",
    "        fp16=config.fp16,\n",
    "        bf16=config.bf16,\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_num_workers=0,\n",
    "        logging_steps=25,\n",
    "        save_steps=100,\n",
    "        eval_steps=100,\n",
    "        save_total_limit=2,\n",
    "        remove_unused_columns=False,\n",
    "        group_by_length=True,\n",
    "        seed=config.seed,\n",
    "    )\n",
    "\n",
    "def test_medical_ai(model, tokenizer, questions=None):\n",
    "    if questions is None:\n",
    "        questions = [\n",
    "            \"What are the early warning signs of heart disease?\",\n",
    "            \"How is diabetes typically diagnosed?\",\n",
    "            \"What should someone do if they suspect they have pneumonia?\",\n",
    "            \"What are the common side effects of blood pressure medications?\",\n",
    "            \"How can someone prevent the spread of infectious diseases?\"\n",
    "        ]\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    for i, q in enumerate(questions, 1):\n",
    "        print(f\"\\nTest {i}: {q}\")\n",
    "        conv = [\n",
    "            {\"role\":\"system\",\"content\":\"You are a knowledgeable medical assistant providing helpful information.\"},\n",
    "            {\"role\":\"user\",\"content\":q},\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(**inputs, max_new_tokens=200, temperature=0.7,\n",
    "                                 do_sample=True, pad_token_id=tokenizer.eos_token_id,\n",
    "                                 repetition_penalty=1.1)\n",
    "        resp = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        print(resp.split(\"\\n\")[-1].strip())\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "def train_medical_ai():\n",
    "    print(\"Starting Medical AI Training Pipeline\")\n",
    "    ds = load_medical_dataset(\"medDataset_processed.csv\")\n",
    "    model, tokenizer = initialize_medical_model()\n",
    "    processed = prepare_training_data(ds, tokenizer)\n",
    "    split = processed.train_test_split(test_size=0.1, seed=config.seed)\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=split[\"train\"],\n",
    "        eval_dataset=split[\"test\"],\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=config.max_seq_length,\n",
    "        args=setup_medical_training(),\n",
    "        packing=False\n",
    "    )\n",
    "    print(\"Training...\")\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"Training completed!\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Training error:\", e)\n",
    "        return None, None\n",
    "    model.save_pretrained(\"medical-ai-lora\")\n",
    "    tokenizer.save_pretrained(\"medical-ai-lora\")\n",
    "    print(\"Saved model to medical-ai-lora\")\n",
    "    print(\"Running tests:\")\n",
    "    test_medical_ai(model, tokenizer)\n",
    "    return model, tokenizer\n",
    "\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        used = torch.cuda.memory_allocated() / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"GPU Memory: {used:.1f}GB used of {total:.1f}GB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Welcome to Medical AI Training!\")\n",
    "    clear_memory()\n",
    "    model, tokenizer = train_medical_ai()\n",
    "    if model:\n",
    "        print(\"SUCCESS! Your Medical AI is ready.\")\n",
    "    else:\n",
    "        print(\"Training failed. Check errors above.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
